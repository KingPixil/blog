<!doctype html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Gradient Descent | Kabir Shah's blog."><meta name="author" content="Kabir Shah"><title>Gradient Descent | Kabir Shah</title><link rel="shortcut icon" href="../img/logoFill.png"/><link href="https://fonts.googleapis.com/css?family=Inconsolata|Lora" rel="stylesheet"><link rel="stylesheet" type="text/css" href="../fonts/fonts.css"/><link rel="stylesheet" type="text/css" href="../css/lib/wing.min.css"/><link rel="stylesheet" type="text/css" href="../css/post.css"/><link rel="stylesheet" type="text/css" href="../css/post-math.css"/><script>!function(e,a,t,n,g,c,o){e.GoogleAnalyticsObject=g,e.ga=e.ga||function(){(e.ga.q=e.ga.q||[]).push(arguments)},e.ga.l=1*new Date,c=a.createElement(t),o=a.getElementsByTagName(t)[0],c.async=1,c.src="https://www.google-analytics.com/analytics.js",o.parentNode.insertBefore(c,o)}(window,document,"script",0,"ga"),ga("create","UA-70792533-7","auto"),ga("send","pageview")</script></head><body><div class="container"><a href="../" id="back"><svg width="37" height="24" viewBox="0 0 37 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>Arrow</title><path id="back-arrow" d="M36.0607 1.06066c.5857-.585786.5857-1.535534 0-2.12132l-9.546-9.54594c-.5858-.5858-1.5355-.5858-2.1213 0-.5858.5858-.5858 1.53553 0 2.12132L32.8787 0l-8.4853 8.48528c-.5858.58579-.5858 1.53552 0 2.12132.5858.5858 1.5355.5858 2.1213 0l9.546-9.54594zM0 1.5h35v-3H0v3z" transform="rotate(180 18.5 6)" fill="#888"/></svg></a><h1 class="post-title">Gradient Descent</h1><h4 class="post-date">Draft</h4><div class="post-content"><p>Neural networks work by getting a set of inputs, multiplying them by a set of weights, and then returning an output. At first, the weights are initialized with random values. Eventually, the neural network refines the weights so that it can return a plausible output for a given input.</p><p>A neural network uses a <em>loss function</em> to find how wrong the output was. The goal of the network is to change the weights in such a way that the loss function returns an output close to zero. A low loss value means that the output for all inputs was incredibly close to the expected output for the inputs.</p><p>A method called <em>gradient descent</em> is often used to refine (&quot;train&quot;) the weights so that the loss value gradually becomes smaller.</p><h3 id="two-variables">Two Variables</h3><p>Instead of jumping right into a 500-dimensional gradient descent example, let&#39;s start out with a simple function that uses two variables, where <span class="mjx-chtml post-math-inline" style="text-align: center"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">x</span></span></span></span></span> is the input, and <span class="mjx-chtml post-math-inline" style="text-align: center"><span class="mjx-math" aria-label="w"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">w</span></span></span></span></span> is the weight.</p><span class="mjx-chtml MJXc-display post-math" style="text-align: center"><span class="mjx-math" aria-label="f(x, w) = xw
"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em">f</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em">(</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">x</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.144em; padding-bottom: 0.519em">,</span></span><span class="mjx-mi MJXc-space1"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">w</span></span><span class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.446em; padding-bottom: 0.593em">)</span></span><span class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.077em; padding-bottom: 0.298em">=</span></span><span class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">x</span></span><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">w</span></span></span></span></span><p>Looking at this, it is fairly straightforward to find how changing <span class="mjx-chtml post-math-inline" style="text-align: center"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">x</span></span></span></span></span> by one changes the output by <span class="mjx-chtml post-math-inline" style="text-align: center"><span class="mjx-math" aria-label="w"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">w</span></span></span></span></span>, and how changing <span class="mjx-chtml post-math-inline" style="text-align: center"><span class="mjx-math" aria-label="w"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">w</span></span></span></span></span> by one changes the output by <span class="mjx-chtml post-math-inline" style="text-align: center"><span class="mjx-math" aria-label="x"><span class="mjx-mrow" aria-hidden="true"><span class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.225em; padding-bottom: 0.298em">x</span></span></span></span></span>.</p></div></div></body></html>